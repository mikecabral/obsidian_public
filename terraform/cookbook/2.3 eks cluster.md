## Discussion

In this example, we were able to start with a completely blank slate and use Terraform’s public module registry to source and create an entire virtual private cloud in Amazon in under an hour. The Terraform registry is a powerful resource for finding the best building blocks possible for your cloud or infrastructure.

Even if you’re not able to utilize a module from the registry due to your team’s policies or procedures, you can view the source code for the modules to see how to implement them on your own. You can also contribute patches or fixes to these modules if you’d like.

# 2.3 Using Public Modules to create an EKS cluster

## Problem

After setting up a Virtual Private Cloud (VPC) within AWS, you’d like to start hosting some applications on Kubernetes. Right now, you have a network based on the previous recipe, but you’d like to stand up an Elastic Kubernetes Service (EKS) cluster and get it configured to use for the first time.

## Solution

Using the public AWS EKS module on the Terraform Registry, you can supercharge your infrastructure with code configuration with exactly what you need to get your Kubernetes cluster running on AWS.

###   
Preparation

We can browse the [Terraform Registry](https://registry.terraform.io/) to find many modules, including the one we’ll be using today, by searching “terraform-aws-modules eks.”

### Steps

Since we’re building off the previous recipe we used for the VPC module, we don’t have to re-specify the AWS provider, or create the VPC module again.

Our `variables.tf` file is mostly set up, but there are more variables we have to add, like the Kubernetes version we want to use, and how we want to configure our worker node pool that are going to host our applications as we add them to our EKS cluster.

First, let’s add these values to our `variables.tf` file that will be necessary for all the required inputs we need for using the EKS module.

```bash
variable "cluster_version" {
  type        = string
  description = "The Kubernetes version for our clusters"
  default     = "1.21"
}
variable "cluster_instance_type" {
  type        = string
  description = "EC2 instance type for the EKS autoscaling group."
  default     = "m5.large"
}
variable "cluster_asg_desired_capacity" {
  type        = number
  description = "The default number of EC2 instances our EKS cluster runs."
  default     = 3
}
variable "cluster_asg_max_size" {
  type        = number
  description = "The maximum number of EC2 instances our EKS cluster will have."
  default     = 5
}
variable "cluster_enabled_log_types" {
  type        = list(string)
  description = "The Kubernetes log types that will be enabled for the EKS cluster."
  default     = ["api", "audit", "authenticator", "controllerManager", "scheduler"]
}
variable "cluster_write_kubeconfig" {
  type        = bool
  description = "Specify if Terraform should output the Kubernetes configuration file. "
  default     = false
}
```

Excellent! Those variables will set us up for success, and now we can continue adding a few more Terraform configuration files to complete this instantiation.

Next, we’ll set up a KMS key for encrypting and securing our secrets within Kubernetes. Please add a `kms.tf` file to your project with the following configuration. This key will be rotated by Amazon and will remove our responsibility from having to ensure regular rotation in a manual sense.

```bash
# https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/kms_key
resource "aws_kms_key" "eks" {
  description             = "EKS Secret Encryption Key"
  deletion_window_in_days = 7
  enable_key_rotation     = true
}
```

Lastly, we’ll set up the EKS cluster by passing the variables we defined, passing the KMS key details of what we created, and working with Terraform’s data sources to authenticate to our EKS cluster.

```bash
data "aws_eks_cluster" "cluster" {
  name = module.eks.cluster_id
}
data "aws_eks_cluster_auth" "cluster" {
  name = module.eks.cluster_id
}
provider "kubernetes" {
  host                   = data.aws_eks_cluster.cluster.endpoint
  cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data)
  token                  = data.aws_eks_cluster_auth.cluster.token
}
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "17.23.0"
  cluster_name              = var.project_name
  cluster_version           = var.cluster_version
  subnets                   = module.vpc.private_subnets
  vpc_id                    = module.vpc.vpc_id
  cluster_enabled_log_types = var.cluster_enabled_log_types
  write_kubeconfig          = var.cluster_write_kubeconfig
  cluster_encryption_config = [
    {
      provider_key_arn = aws_kms_key.eks.arn
      resources        = ["secrets"]
    }
  ]
  worker_groups = [
    {
      asg_desired_capacity = var.cluster_asg_desired_capacity
      asg_max_size         = var.cluster_asg_max_size
      instance_type        = var.cluster_instance_type
    }
  ]
}
```

Now that we have all our Terraform configurations in place, we can call terraform init to download our dependencies and create our Terraform lockfile.

```bash
terraform init
```
Terraform has been successfully initialized!

You can run a `terraform plan` to validate your configuration has been specified correctly, and you can run a `terraform apply` if you’d like to stand up your cluster.

If you end up applying this code, you can remove it by running terraform destroy.